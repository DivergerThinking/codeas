{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "def is_path_matched(path: str, patterns: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a path matches any of the given patterns.\n",
    "    \"\"\"\n",
    "    return any(re.match(pattern, path) for pattern in patterns)\n",
    "\n",
    "def convert_to_regex(pattern: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a gitignore-like pattern to a regex pattern.\n",
    "    \"\"\"\n",
    "    pattern = pattern.replace(\".\", r\"\\.\")\n",
    "    pattern = pattern.replace(\"*\", \".*\")\n",
    "    pattern = pattern.replace(\"?\", \".\")\n",
    "    return f\"^{pattern}$\"\n",
    "\n",
    "def process_dir_patterns(dir_patterns: Optional[List[str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Process directory patterns and convert them to regex patterns.\n",
    "    \"\"\"\n",
    "    if dir_patterns is None:\n",
    "        return []\n",
    "    return [convert_to_regex(pattern) for pattern in dir_patterns]\n",
    "\n",
    "def should_process_directory(dirpath: str, include_patterns: List[str], exclude_patterns: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if a directory should be processed based on include and exclude patterns.\n",
    "    \"\"\"\n",
    "    if include_patterns and not is_path_matched(dirpath, include_patterns):\n",
    "        return False\n",
    "    if exclude_patterns and is_path_matched(dirpath, exclude_patterns):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def read_repository_files(\n",
    "    path: str,\n",
    "    include_dir: Optional[List[str]] = None,\n",
    "    exclude_dir: Optional[List[str]] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Read all files in a repository, optionally filtering by included or excluded directories.\n",
    "\n",
    "    Args:\n",
    "    path (str): The path of the repository.\n",
    "    include_dir (Optional[List[str]]): List of directories to include (supports regex-like patterns).\n",
    "    exclude_dir (Optional[List[str]]): List of directories to exclude (supports regex-like patterns).\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of file paths found in the repository.\n",
    "    \"\"\"\n",
    "    include_patterns = process_dir_patterns(include_dir)\n",
    "    exclude_patterns = process_dir_patterns(exclude_dir)\n",
    "    \n",
    "    files = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        if should_process_directory(dirpath, include_patterns, exclude_patterns):\n",
    "            for filename in filenames:\n",
    "                files.append(os.path.join(dirpath, filename))\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../.git/objects/01/142595d7c2fd0a24d10427e56f9d885d51451f\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte"
     ]
    }
   ],
   "source": [
    "with open(\"../.git/objects/01/142595d7c2fd0a24d10427e56f9d885d51451f\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_repository_files(\"..\", exclude_dir=[\".*\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"a\":1,\"b\":2}\n",
    "d.pop(list(d.keys())[0])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix asyncio in notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload module in notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeag.core.agent import Agent\n",
    "from codeag.core.configs.api_params import GPT35_BASE_PARAMS, GPT4_BASE_PARAMS, GPT4_NO_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(repo_path=\"../../SWE-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract documentation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_extractor = \"\"\"\n",
    "I want to generate some documentation for an entire repository.\n",
    "In order to do that, I need to define which sections the documentation should have based on the repository content.\n",
    "Given that the repository is very large, I first want to give each file a label that reflects its content.\n",
    "Based on these labels, I will define the sections of the documentation. \n",
    "Here are some examples of relevant labels that could be used to define the sections of the documentation:\n",
    "configuration, deployment, security, authentification, front-end, back-end, database, testing, CI/CD, etc.\n",
    "These are just examples, don't hesitate to include additional labels more specific to what the file does\n",
    "\n",
    "*IMPORTANT*:\n",
    "Each file can have multiple labels.\n",
    "If you think a file is not relevant for the documentation, don't give it any label. \n",
    "\n",
    "Define the labels for the following file:\n",
    "{files_content}\n",
    "\n",
    "Return your answer in JSON format as such:\n",
    "{{\"labels\": [\"label1\", \"label2\", \"label3\"]}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = agent.get_messages(prompt=label_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046892"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.calculate_cost(messages, openai_params=GPT35_BASE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = agent.generate_responses(messages, GPT35_BASE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_labels = {}\n",
    "for path, response in zip(messages.keys(),responses):\n",
    "    files_labels[path] = eval(response[\"content\"])[\"labels\"]\n",
    "\n",
    "def retrieve_files(labels):\n",
    "    files = []\n",
    "    for path, labels_ in files_labels.items():\n",
    "        if any(label in labels_ for label in labels):\n",
    "            files.append(path)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../SWE-agent/sweagent/agent/commands.py']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_files([\"documentation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(responses):\n",
    "    label_counts = {}\n",
    "    for response in responses:\n",
    "        labels = eval(response[\"content\"])['labels']\n",
    "        for label in labels:\n",
    "            if label in label_counts:\n",
    "                label_counts[label] += 1\n",
    "            else:\n",
    "                label_counts[label] = 1\n",
    "    return label_counts\n",
    "\n",
    "label_count = count_labels(responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_label_count = {k: v for k, v in sorted(label_count.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deployment': 29,\n",
       " 'testing': 28,\n",
       " 'front-end': 21,\n",
       " 'back-end': 20,\n",
       " 'configuration': 14,\n",
       " 'UI': 7,\n",
       " 'CI/CD': 6,\n",
       " 'React': 5,\n",
       " 'security': 4,\n",
       " 'react': 4,\n",
       " 'scripting': 2,\n",
       " 'routing': 2,\n",
       " 'component': 2,\n",
       " 'docker': 1,\n",
       " 'bash': 1,\n",
       " 'bash script': 1,\n",
       " 'performance': 1,\n",
       " 'state management': 1,\n",
       " 'javascript': 1,\n",
       " 'syntax-highlighting': 1,\n",
       " 'authentification': 1,\n",
       " 'data-processing': 1,\n",
       " 'parsing': 1,\n",
       " 'dataclass': 1,\n",
       " 'abstract class': 1,\n",
       " 'json parsing': 1,\n",
       " 'error handling': 1,\n",
       " 'documentation': 1,\n",
       " 'socketio': 1,\n",
       " 'web interface': 1,\n",
       " 'data processing': 1,\n",
       " 'data analysis': 1,\n",
       " 'csv generation': 1,\n",
       " 'experiment results': 1,\n",
       " 'data visualization': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count_str = \"\"\n",
    "\n",
    "for label, count in sorted_label_count.items():\n",
    "    label_count_str += f\"{label}: {count}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_descriptions = \"\"\"\n",
    "I want to generate some documentation for an entire repository.\n",
    "In order to do that, I need to define which sections the documentation should have based on the repository content.\n",
    "Given that the repository is very large, I first want to extract short and long descriptions about what each file in the repo does.\n",
    "Based on these descriptions, I will define the sections of the documentation.\n",
    "\n",
    "*IMPORTANT*:\n",
    "The short descriptions should be as concise as possible, with a maximum of around 50 tokens.\n",
    "The longer descriptions should use bullet points to list the main points of the file content.\n",
    "\n",
    "Write the descriptions for following file:\n",
    "{files_content}\n",
    "\n",
    "Return your answer in JSON format as such:\n",
    "{{\n",
    "    \"description\": \"short description here\",\n",
    "    \"details\": \"long description here\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_messages = agent.get_messages(prompt=extract_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0453485"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.calculate_cost(descriptions_messages, openai_params=GPT35_BASE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = agent.generate_responses(descriptions_messages, GPT35_BASE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_responses = {}\n",
    "for path, response in zip(descriptions_messages.keys(),responses):\n",
    "    descriptions_responses[path] = eval(response[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_descriptions(paths = None):\n",
    "    full_descriptions_str = \"\"\n",
    "    for path, response in descriptions_responses.items():\n",
    "        if paths and path not in paths:\n",
    "            pass\n",
    "        else:\n",
    "            full_descriptions_str += f'{path}:\\n{response[\"description\"]}\\n{response[\"details\"]}\\n\\n'\n",
    "    return full_descriptions_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6323"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(full_descriptions_str, model=GPT35_BASE_PARAMS[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_directories = \"\"\"\n",
    "I want to document the main directories found inside a repository.\n",
    "A description for each file found inside this repository has been generated.\n",
    "\n",
    "Here are the descriptions for each file:\n",
    "{descriptions_str}\n",
    "\n",
    "Write a brief description for the most relevant directories found in the repository.\n",
    "\n",
    "**IMPORTANT**:\n",
    "The descriptions should be as concise as possible, with a maximum of around 50 tokens.\n",
    "Include as many directories and subdirectories as you can.\n",
    "\n",
    "Return your answer in JSON format as such:\n",
    "\"directory_name_1\": \"directory description here\", \n",
    "\"directory_name_2\": \"directory description here\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_directories_prompt = document_directories.format(descriptions_str=descriptions_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = agent.get_messages(prompt=document_directories_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = agent.generate_responses(messages, GPT4_BASE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"SWE-agent\": \"Root directory containing scripts, configuration files, and main components for the SWE-agent.\",\n",
      "    \"SWE-agent/docker\": \"Contains helper scripts for Docker setup and management.\",\n",
      "    \"SWE-agent/config/commands\": \"Shell scripts for various command functionalities like editing, searching, and cursor manipulation.\",\n",
      "    \"SWE-agent/inspector\": \"Files for handling HTTP requests and viewing file content in the trajectory viewer.\",\n",
      "    \"SWE-agent/tests\": \"Test files and fixtures for validating different functionalities of the SWE-agent.\",\n",
      "    \"SWE-agent/tests/test_data/trajectories\": \"Contains test trajectory files used for testing.\",\n",
      "    \"SWE-agent/sweagent/frontend/src\": \"Source files for the frontend React application.\",\n",
      "    \"SWE-agent/sweagent/frontend/src/components\": \"React components for the frontend UI.\",\n",
      "    \"SWE-agent/sweagent/frontend/src/components/panels\": \"React components for displaying different panels in the frontend.\",\n",
      "    \"SWE-agent/sweagent/frontend/src/components/utils/icons\": \"React components for rendering icons.\",\n",
      "    \"SWE-agent/utils\": \"Utility modules for handling configuration settings.\",\n",
      "    \"SWE-agent/agent\": \"Classes and functions for agent behavior, models, and parsing.\",\n",
      "    \"SWE-agent/environment\": \"Classes and functions for interacting with the SWE-bench environment.\",\n",
      "    \"SWE-agent/api\": \"Server-side code and hooks for the SWE-agent API.\",\n",
      "    \"SWE-agent/scripts\": \"Shell scripts for running, evaluating, and managing the SWE-agent.\",\n",
      "    \"SWE-agent/evaluation\": \"Functions and scripts for aggregating and running evaluation results.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(responses[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define documentation sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sections = \"\"\"\n",
    "I want to generate some documentation for an entire repository.\n",
    "To do so, I want to define the sections of the documentation based on some labels that were generated for each file.\n",
    "Here are the labels that were generated and the number of files that have each label:\n",
    "{label_count}\n",
    "\n",
    "Define the sections of the documentation that you think are most relevant to the codebase based on the labels.\n",
    "Don't use all of the labels, only those you think are relevant to writing the documentation.\n",
    "\n",
    "Return your results in JSON format as follows:\n",
    "\"sections\": [\"section1\", \"section2\", \"section3\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sections_prompt = doc_sections.format(label_count=label_count_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = agent.get_messages(prompt=doc_sections_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = agent.generate_responses(messages, GPT4_BASE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = eval(responses[\"content\"])[\"sections\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_str = \"\"\n",
    "for section in sections:\n",
    "    sections_str += f\"- {section}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Deployment\n",
      "- Testing\n",
      "- Front-End\n",
      "- Back-End\n",
      "- Configuration\n",
      "- UI\n",
      "- CI/CD\n",
      "- Security\n",
      "- Scripting\n",
      "- Routing\n",
      "- Components\n",
      "- Performance\n",
      "- State Management\n",
      "- Documentation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sections_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for files with specific labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_files = \"\"\"\n",
    "I want to document different sections from a repository.\n",
    "In order to do so, I need to retrieve the relevant files for each section using the labels that were generated for each file.\n",
    "\n",
    "Here are the different labels that were generated and the number of files that have each label:\n",
    "{label_count_str}\n",
    "\n",
    "Here are the different sections that were defined for the documentation:\n",
    "{sections_str}\n",
    "\n",
    "For each section, identify the relevant labels that should be used for searching the files that belong to that section.\n",
    "\n",
    "**IMPORTANT**:\n",
    "Different sections may use the same labels.\n",
    "\n",
    "Return your answer in JSON format as such:\n",
    "section1: [\"label1\", \"label2\", \"label3\"],\n",
    "section2: [\"label4\", \"label5\", \"label1\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_files_prompt = search_files.format(label_count_str=label_count_str, sections_str=sections_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = agent.get_messages(prompt=search_files_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = agent.generate_responses(messages, GPT4_BASE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_labels = eval(responses[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_files(section):\n",
    "    labels = section_labels[section]\n",
    "    return retrieve_files(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sections_contents():\n",
    "    section_contents = {}\n",
    "    for section in section_labels.keys():\n",
    "        section_content_str = \"\"\n",
    "        file_paths = get_section_files(section)\n",
    "        section_content_str = get_full_descriptions(file_paths)\n",
    "        section_contents[section] = section_content_str\n",
    "    return section_contents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_contents = get_sections_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../SWE-agent/inspector/fileViewer.js:\n",
      "JavaScript file containing functions to fetch and display files and their content in a web viewer\n",
      "['Defines variables to store current file name, directory, and timeout IDs for pending operations', 'Includes functions to get the base URL, fetch files list, get role text, view file content, refresh current file, and fetch directory info', 'Utilizes fetch API to retrieve data from server and display file content in a structured manner', 'Implements role mapping for different types of agents and content display', 'Handles error cases and updates UI elements accordingly', 'Executes necessary functions on window load to initialize file fetching and directory info retrieval']\n",
      "\n",
      "../../SWE-agent/inspector/static.py:\n",
      "Python module containing functions for generating static HTML viewers for trajectory files.\n",
      "- Defines a template HTML structure for displaying conversation history from trajectory files.\n",
      "- Loads style sheet and handles exceptions if style sheet loading fails.\n",
      "- Defines functions for loading file content, creating file path tree, and saving static viewer.\n",
      "- Includes a function for finding relative path between two directories.\n",
      "- Provides a function for saving static viewers for all trajectory files in a given directory.\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/reportWebVitals.js:\n",
      "Function to report web vitals performance metrics\n",
      "- Function that reports web vitals performance metrics\n",
      "- Takes a callback function as input\n",
      "- Imports web-vitals library dynamically\n",
      "- Calls functions to get Core Web Vitals metrics (CLS, FID, FCP, LCP, TTFB)\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/Run.js:\n",
      "React component for running a simulation with agent and environment feeds\n",
      "- Contains functions for handling form submission, socket updates, and error messages\n",
      "- Uses socket.io for real-time communication\n",
      "- Renders agent and environment feeds with log panel\n",
      "- Allows for stopping and restarting the simulation\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/index.js:\n",
      "React index file for SWE-agent frontend\n",
      "This file serves as the entry point for the SWE-agent frontend application. It imports necessary dependencies such as React, react-dom, react-router-dom, and bootstrap. The main function of this file is to render the main App component within a BrowserRouter component. Additionally, it includes a function call to reportWebVitals for measuring performance in the app.\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/components/panels/EnvFeed.js:\n",
      "React component for displaying environmental feed messages in a terminal-like interface.\n",
      " - Renders a MacBar component with title 'Terminal' and a terminal logo\n",
      " - Scrolls to the bottom of the feed when updated\n",
      " - Maps over feed items to render EnvMessage components\n",
      " - Handles mouse enter and leave events for each message\n",
      " - Highlights messages based on a specified step\n",
      " - Contains CSS classes for styling the feed\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/components/panels/LogPanel.js:\n",
      "React component for displaying and copying log content\n",
      "- Displays log content in a scrollable panel\n",
      "- Allows users to copy log content to clipboard\n",
      "- Utilizes MacBar component for styling\n",
      "- Includes workspace logo and copy button\n",
      "- Dynamically updates based on log data and computing status\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/components/panels/AgentFeed.js:\n",
      "React component for displaying agent feed messages\n",
      "- Renders a feed of messages from the agent\n",
      "- Includes a MacBar component with title 'Thoughts'\n",
      "- Utilizes Message component to display individual messages\n",
      "- Automatically scrolls to the bottom of the feed on update\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/components/Header.js:\n",
      "React component for the header section of the SWE agent frontend\n",
      " - React component that renders the header section of the SWE agent frontend\n",
      " - Imports necessary dependencies such as React, Link, and logo\n",
      " - Contains a link to the SWE agent GitHub repository\n",
      " - Displays the SWE agent logo with a link to the GitHub repository\n",
      " - Commented out code for additional links/buttons (Home and Discord)\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/components/utils/icons/ExpandIcon.js:\n",
      "React component for rendering an expand icon in SVG format.\n",
      "This file contains a React functional component called ExpandIcon, which is used to render an expand icon in SVG format. The component takes in props for fillColor, height, and style. The SVG icon consists of two paths that form an arrow pointing upwards. The default props for fillColor is black, height is 24px, and style is an empty object. The SVG paths are defined with specific coordinates to create the desired arrow shape.\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/components/AgentMessage.js:\n",
      "React component for displaying agent messages with formatting and highlighting\n",
      "- React component that displays agent messages with specific formatting and highlighting\n",
      "- Imports necessary styles and icons\n",
      "- Receives props such as item, handleMouseEnter, handleMouseLeave, isHighlighted, and feedRef\n",
      "- Dynamically applies CSS classes based on item properties\n",
      "- Renders message content with Gear icon if type is not 'thought'\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/components/controls/LRunControl.js:\n",
      "React component for controlling SWE-agent runs with tabs for different settings\n",
      "- Contains tabs for 'Problem Source', 'Model', 'Environment', and 'Extra Settings'\n",
      "- Allows user to input problem statement, repository, model name, and environment setup\n",
      "- Provides options for test runs and links to GitHub repository\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/components/MacBar.js:\n",
      "React component for rendering a Mac-style window top bar\n",
      "- React component named MacBar\n",
      "- Renders a Mac-style window top bar with a title and logo\n",
      "- Accepts props for title, logo, and dark mode\n",
      "- Applies dark mode styling if dark prop is true\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/components/EnvMessage.js:\n",
      "React component for displaying environment messages with syntax highlighting\n",
      "['Defines a functional component called EnvMessage', 'Imports necessary dependencies such as React, SyntaxHighlighter, and stylesheets', 'Contains a helper function capitalizeFirstLetter', 'Renders different types of environment messages based on item type', 'Applies syntax highlighting for command, output, and diff message types', 'Handles mouse events for highlighting and displaying message details']\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/components/Footer.js:\n",
      "Footer component displaying copyright information\n",
      "- React component for displaying the footer of the website\n",
      "- Contains copyright information for the year 2024\n",
      "- Authors listed: John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, Ofir Press\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/App.test.js:\n",
      "Test file for rendering the App component and checking if 'learn react' link is present\n",
      "- Contains a test case for rendering the App component\n",
      "- Uses @testing-library/react for testing\n",
      "- Checks if 'learn react' link is present on the rendered component\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/setupTests.js:\n",
      "File for setting up Jest tests with custom matchers for DOM nodes\n",
      "- Adds custom jest matchers for asserting on DOM nodes\n",
      "- Enables assertions like expect(element).toHaveTextContent(/react/i)\n",
      "- More information available at https://github.com/testing-library/jest-dom\n",
      "\n",
      "../../SWE-agent/sweagent/frontend/src/App.js:\n",
      "React application main component rendering Header, Run, and Footer components\n",
      "- App.js is the main component of the React application\n",
      "- It imports Header, Run, and Footer components\n",
      "- It uses react-router-dom for routing\n",
      "- It renders Header, Run, and Footer components within a div element\n",
      "\n",
      "../../SWE-agent/sweagent/environment/swe_env.py:\n",
      "Python file containing classes and functions to interact with the SWE-bench environment using Gym. It handles communication with a docker container, environment setup, task initialization, and submission of results.\n",
      "The 'swe_env.py' file contains classes and functions for interacting with the SWE-bench environment. Here are the main points covered in the file:\n",
      "- Definition of the EnvironmentArguments dataclass for configuring environment setup instructions.\n",
      "- Definition of the EnvHook class for handling environment initialization and closure.\n",
      "- Definition of the SWEEnv class, a Gym environment for SWE-bench tasks, which communicates with a docker container.\n",
      "- Methods for resetting the environment, running actions in the environment, and closing the environment.\n",
      "- Functions for creating a conda environment, installing dependencies, and running shell scripts in the container.\n",
      "- Methods for handling interruptions, opening pull requests, and adding custom commands to the container.\n",
      "\n",
      "../../SWE-agent/sweagent/api/hooks.py:\n",
      "This file contains hooks for updating the web interface using socketio in the SWE-agent repository.\n",
      "['Defines a StreamToSocketIO class that writes messages to a log stream', 'Defines a WebUpdate class that communicates with socketio to update log, banner, agent feed, and environment feed', 'Defines MainUpdateHook class that updates the web interface on start, end, and instance completion events', 'Defines AgentUpdateHook class that updates the web interface on actions generated and sub actions started/executed events', 'Defines EnvUpdateHook class that updates the web interface on environment close event']\n",
      "\n",
      "../../SWE-agent/start_web_ui.sh:\n",
      "Bash script to start the web UI for the SWE agent\n",
      " - Script to start the web UI for the SWE agent\n",
      " - Stops the react server if running\n",
      " - Installs npm packages and starts the server\n",
      " - Provides instructions for accessing the UI\n",
      " - Logs errors to web_api.log\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(section_contents[\"Front-End\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeag.core.utils.costs import count_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1939"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(section_contents[\"Front-End\"],model=GPT35_BASE_PARAMS[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate documentation sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_sections = \"\"\"\n",
    "I want to generate some documentation for an entire repository.\n",
    "I want to do this by generating each section of the documentation separately.\n",
    "\n",
    "Here are the different sections that were defined for the documentation:\n",
    "{sections_str}\n",
    "\n",
    "Generate the documentation for the following section:\n",
    "{section}\n",
    "\n",
    "Here are some file information that should be relevant for that section:\n",
    "{section_content}\n",
    "\n",
    "**IMPORTANT**:\n",
    "Not all files are necessarily relevant to the section.\n",
    "Don't try to include all of the file's information inside the documentation, only focus on those that are relevant.\n",
    "The documentation you generate should be broken down into different subsections whenever necessary.\n",
    "Follow an html-like structure for the documentation, using \"h1\", \"h2\", \"h3\" as section titles and \"p\" as content.\n",
    "\n",
    "Return your answer in JSON format following this structure:\n",
    "\"0 - h1\": \"section title here\",\n",
    "\"1 - h2\": \"subsection title here\",\n",
    "\"2 - p\": \"content here\"\n",
    "\"3 - h2\": \"subsection title here\",\n",
    "\"4 - h3\": \"subsubsection title here\",\n",
    "\"5 - p\": \"content here\"\n",
    "\n",
    "NOTE: the 0 - 1 - 2 - 3 - 4 - 5 are just indexes to make the JSON keys unique.\n",
    "\n",
    "If you think that not enough context is provided to generate the documentation, return the answer as follows:\n",
    "\"error\": \"not enough context provided\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {}\n",
    "for section in sections:\n",
    "    messages[section] = agent.get_messages(prompt=document_sections.format(sections_str=sections_str, section=section, section_content=section_contents[section]))\n",
    "    # messages = agent.get_messages(prompt=document_sections_prompt)\n",
    "    # responses = agent.generate_responses(messages, GPT4_BASE_PARAMS)\n",
    "    # print(responses[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = agent.generate_responses(messages, GPT4_BASE_PARAMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_to_md(response_content):\n",
    "    for tag, content in response_content.items():\n",
    "        if \"h1\" in tag:\n",
    "            print(f\"# {content}\")\n",
    "        elif \"h2\" in tag:\n",
    "            print(f\"## {content}\")\n",
    "        elif \"h3\" in tag:\n",
    "            print(f\"### {content}\")\n",
    "        elif \"p\" in tag:\n",
    "            print(f\"{content}\\n\")\n",
    "        else:\n",
    "            print(\"Error: unrecognized tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Deployment\n",
      "## Overview\n",
      "This section covers the deployment process for the SWE-agent, including setting up Docker images, building and pushing official Docker images, and running the software agent. The deployment scripts ensure that the software is correctly built, packaged, and deployed to the appropriate environments.\n",
      "\n",
      "## Setup Script\n",
      "### setup.sh\n",
      "The `setup.sh` script is used to build Docker images for the SWE-agent and evaluation components. It follows bash strict mode and sets up the Docker image for SWE-agent by building it with the specified Dockerfile and setting the TARGETARCH variable. It then proceeds to set up the Docker image for evaluation using a separate Dockerfile. Finally, the script prints 'Done with setup!' to indicate completion.\n",
      "\n",
      "## Building and Pushing Docker Images\n",
      "### release_dockerhub.sh\n",
      "The `release_dockerhub.sh` script is responsible for building official Docker images and pushing them to DockerHub after user confirmation. It builds images for `swe-agent`, `swe-eval`, and `swe-agent-run`. The user must provide a username and version number, and the script validates the version number format. It uses `docker buildx` for multi-platform builds, creates tags in Git, and pushes to GitHub if the version is not 'latest'.\n",
      "\n",
      "## Running the Software Agent\n",
      "### run.py\n",
      "The `run.py` script is a Python script for running inference on issues from GitHub repositories and applying patches locally. It handles actions like opening PRs and skipping instances with existing trajectories. The script imports necessary modules and packages, defines dataclasses for script arguments, and includes hooks for saving and applying patches, opening PRs, and controlling script flow. It defines a Main class for running the script, parses command line arguments, and executes the main function.\n",
      "\n",
      "## Helper Scripts\n",
      "### getconda.sh\n",
      "The `getconda.sh` script is a helper script to download the appropriate Miniconda version based on the architecture specified. It handles x86_64 and aarch64 architectures, downloading Miniconda3-py39_23.11.0-1 for the respective architecture. This script is used in multi-platform builds within Docker containers.\n",
      "\n",
      "### build_deploy.sh\n",
      "The `build_deploy.sh` script is used for building and deploying the software agent. It uses Python to build the software and uploads the built software to the PyPI repository. There is also an option to upload to the test PyPI repository.\n",
      "\n",
      "## Running and Evaluating the Agent\n",
      "### run_and_eval.sh\n",
      "The `run_and_eval.sh` script is used to run and evaluate the agent multiple times. It takes user-defined variables such as suffix, template, dataset path, and number of runs. The script iterates through the specified number of runs, executing commands to run the agent and evaluate its predictions using the `evaluation.py` script.\n",
      "\n",
      "## Removing Docker Containers\n",
      "### remove_all_containers.sh\n",
      "The `remove_all_containers.sh` script is a bash script used to remove all Docker containers in the system. It uses the `docker rm` command with the `-f` flag to force removal. The command `docker ps -aq` lists all container IDs which are then passed to `docker rm`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parse_json_to_md(eval(responses[0][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify files to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6323"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(full_descriptions_str, model=GPT35_BASE_PARAMS[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_test_files = \"\"\"\n",
    "I want to generate some tests for a repository.\n",
    "In order to do that, I first need to identify which files from the repository are relevant for testing.\n",
    "\n",
    "The relevance of these tests should be categorized into three main categories:\n",
    "- High: files that are critical for the application and should be tested thoroughly\n",
    "- Medium: files that are important for the application but not critical\n",
    "- Low: files that are not critical and can be tested with less priority\n",
    "\n",
    "Here are the descriptions for each file:\n",
    "{full_descriptions_str}\n",
    "\n",
    "**IMPORTANT**:\n",
    "Focus on files which are relevant for testing and relatively easy to test (i.e unit testing).\n",
    "Files that are more difficult to test (UI, integration tests, scripts, etc.) should be excluded or ranked as low priority.\n",
    "IGNORE TEST FILES.\n",
    "\n",
    "Return your answer in JSON format as such:\n",
    "High: [\"file1\", \"file2\", \"file3\"],\n",
    "Medium: [\"file4\", \"file5\", \"file6\"],\n",
    "Low: [\"file7\", \"file8\", \"file9\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_test_files_prompt = identify_test_files.format(full_descriptions_str=full_descriptions_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = agent.get_messages(prompt=identify_test_files_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03267"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.calculate_cost(messages, openai_params=GPT4_BASE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = agent.generate_responses(messages, GPT4_BASE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_test = eval(responses[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High\n",
      "\t- ../../SWE-agent/run.py\n",
      "\t- ../../SWE-agent/inspector/server.py\n",
      "\t- ../../SWE-agent/sweagent/agent/models.py\n",
      "\t- ../../SWE-agent/sweagent/agent/agents.py\n",
      "\t- ../../SWE-agent/sweagent/agent/parsing.py\n",
      "\t- ../../SWE-agent/sweagent/environment/swe_env.py\n",
      "\t- ../../SWE-agent/sweagent/environment/utils.py\n",
      "Medium\n",
      "\t- ../../SWE-agent/sweagent/api/server.py\n",
      "\t- ../../SWE-agent/sweagent/api/hooks.py\n",
      "\t- ../../SWE-agent/sweagent/api/utils.py\n",
      "\t- ../../SWE-agent/sweagent/utils/config.py\n",
      "\t- ../../SWE-agent/sweagent/agent/commands.py\n",
      "\t- ../../SWE-agent/sweagent/agent/history_processors.py\n",
      "\t- ../../SWE-agent/evaluation/aggregate_results.py\n",
      "\t- ../../SWE-agent/evaluation/evaluation.py\n",
      "Low\n",
      "\t- ../../SWE-agent/setup.sh\n",
      "\t- ../../SWE-agent/release_dockerhub.sh\n",
      "\t- ../../SWE-agent/docker/getconda.sh\n",
      "\t- ../../SWE-agent/build_deploy.sh\n",
      "\t- ../../SWE-agent/config/commands/edit_linting.sh\n",
      "\t- ../../SWE-agent/config/commands/defaults.sh\n",
      "\t- ../../SWE-agent/config/commands/search.sh\n",
      "\t- ../../SWE-agent/config/commands/cursors_defaults.sh\n",
      "\t- ../../SWE-agent/config/commands/cursors_edit_linting.sh\n",
      "\t- ../../SWE-agent/config/commands/_split_string.py\n",
      "\t- ../../SWE-agent/inspector/fileViewer.js\n",
      "\t- ../../SWE-agent/inspector/static.py\n",
      "\t- ../../SWE-agent/run_replay.py\n",
      "\t- ../../SWE-agent/make_demos/convert_traj_to_demo.py\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/reportWebVitals.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/Run.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/index.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/components/panels/EnvFeed.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/components/panels/LogPanel.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/components/panels/AgentFeed.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/components/Header.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/components/utils/icons/ExpandIcon.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/components/AgentMessage.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/components/controls/LRunControl.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/components/MacBar.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/components/EnvMessage.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/components/Footer.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/App.test.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/setupTests.js\n",
      "\t- ../../SWE-agent/sweagent/frontend/src/App.js\n",
      "\t- ../../SWE-agent/scripts/run_from_url.sh\n",
      "\t- ../../SWE-agent/scripts/run_and_eval.sh\n",
      "\t- ../../SWE-agent/scripts/run.sh\n",
      "\t- ../../SWE-agent/scripts/remove_all_containers.sh\n",
      "\t- ../../SWE-agent/scripts/run_jsonl.sh\n",
      "\t- ../../SWE-agent/scripts/run_replay.sh\n",
      "\t- ../../SWE-agent/evaluation/run_eval.sh\n",
      "\t- ../../SWE-agent/start_web_ui.sh\n"
     ]
    }
   ],
   "source": [
    "for priority, files in files_to_test.items():\n",
    "    print(f\"{priority}\")\n",
    "    for file in files:\n",
    "        print(f\"\\t- {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Extract test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_test_cases = \"\"\"\n",
    "I want to generate some tests for an entire repository.\n",
    "For each of the file in the repository, I first want to define the different test cases (i.e. behaviors to cover) that should be implemented.\n",
    "The tests will then be generated based on these test cases.\n",
    "\n",
    "Define some test cases for the following file:\n",
    "{file_content}\n",
    "\n",
    "**IMPORTANT**:\n",
    "Test cases should be as specific as possible and written in a concise way with a maximum of around 50 tokens.\n",
    "\n",
    "Return your answer in JSON format as such:\n",
    "{{\n",
    "    \"test_file\": \"yes\",\n",
    "    \"test_cases\": {{\n",
    "        \"Class or Function name 1\": [\"test case description\", \"test case description\", \"test case description\"],\n",
    "        \"Class or Function name 2 \": [\"test case description\", \"test case description\", \"test case description\"],\n",
    "    }}\n",
    "}}\n",
    "\n",
    "In case you think the file is not relevant for testing, return it as such:\n",
    "{{\n",
    "    \"test_file\": \"no\",\n",
    "    \"test_cases\": {{}}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI want to generate some tests for an entire repository.\\nFor each of the file in the repository, I first want to define the different test cases (i.e. behaviors to cover) that should be implemented.\\nThe tests will then be generated based on these test cases.\\n\\nDefine some test cases for the following file:\\nfrom __future__ import annotations\\n\\nfrom sweagent import CONFIG_DIR\\n\\ntry:\\n    import rich\\nexcept ModuleNotFoundError as e:\\n    msg = (\\n        \"You probably either forgot to install the dependencies \"\\n        \"or forgot to activate your conda or virtual environment.\"\\n    )\\n    raise RuntimeError(msg) from e\\nimport json\\nimport logging\\nimport os\\nimport re\\nimport subprocess\\nimport traceback\\nfrom typing import Any\\n\\nimport rich.console\\nimport rich.markdown\\nimport rich.panel\\n\\ntry:\\n    from rich_argparse import RichHelpFormatter\\nexcept ImportError:\\n    msg = \"Please install the rich_argparse package with `pip install rich_argparse`.\"\\n    raise ImportError(msg)\\nfrom dataclasses import dataclass\\nfrom getpass import getuser\\nfrom pathlib import Path\\n\\nimport yaml\\nfrom rich.logging import RichHandler\\nfrom rich.markdown import Markdown\\nfrom simple_parsing import parse\\nfrom simple_parsing.helpers.flatten import FlattenedAccess\\nfrom simple_parsing.helpers.serialization.serializable import FrozenSerializable\\nfrom swebench import KEY_INSTANCE_ID, KEY_MODEL, KEY_PREDICTION\\nfrom unidiff import PatchSet\\n\\nfrom sweagent.agent.agents import Agent, AgentArguments\\nfrom sweagent.agent.models import ModelArguments\\nfrom sweagent.environment.swe_env import EnvironmentArguments, SWEEnv\\nfrom sweagent.environment.utils import (\\n    InvalidGithubURL,\\n    get_associated_commit_urls,\\n    get_data_path_name,\\n    get_gh_issue_data,\\n    parse_gh_issue_url,\\n)\\n\\n__doc__: str = \"\"\" Run inference. Usage examples:\\n\\n```bash\\n# Run over a github issue:\\npython run.py --model_name \"gpt4\" --data_path \"https://github.com/pvlib/pvlib-python/issues/1603\" --config_file \"config/default_from_url.yaml\"\\n# Apply a patch in a local repository to an issue specified as Markdown file and run a custom installer script in the container\\npython run.py --model_name \"gpt4\" --data_path \"/path/to/my_issue.md\" --repo_path \"/path/to/my/local/repo\" --environment_setup \"/path/to/setup.sh\" --config_file \"config/default_from_url.yaml\" --apply_patch_locally\\n```\\n\\n**For more information**: https://princeton-nlp.github.io/SWE-agent/usage/cl_tutorial/\\n\"\"\"\\n\\nhandler = RichHandler(show_time=False, show_path=False)\\nhandler.setLevel(logging.DEBUG)\\nlogger = logging.getLogger(\"run_dev\")\\nlogger.setLevel(logging.DEBUG)\\nlogger.addHandler(handler)\\nlogger.propagate = False\\nlogging.getLogger(\"simple_parsing\").setLevel(logging.WARNING)\\n\\n\\n@dataclass(frozen=True)\\nclass ActionsArguments(FlattenedAccess, FrozenSerializable):\\n    \"\"\"Run real-life actions (opening PRs, etc.) if we can solve the issue.\"\"\"\\n\\n    # Open a PR with the patch if we can solve the issue\\n    open_pr: bool = False\\n    # When working with local repository: Apply patch\\n    apply_patch_locally: bool = False\\n    # Option to be used with open_pr: Skip action if there are already commits claiming\\n    # to fix the issue. Please only set this to False if you are sure the commits are\\n    # not fixes or if this is your own repository!\\n    skip_if_commits_reference_issue: bool = True\\n    # OBSOLETE. Do not use, will raise error. Please specify --repo_path instead.\\n    push_gh_repo_url: str = \"\"\\n\\n    def __post_init__(self):\\n        if self.push_gh_repo_url:\\n            msg = \"push_gh_repo_url is obsolete. Use repo_path instead\"\\n            raise ValueError(msg)\\n\\n\\n@dataclass(frozen=True)\\nclass ScriptArguments(FlattenedAccess, FrozenSerializable):\\n    \"\"\"Configure the control flow of the run.py script\"\"\"\\n\\n    environment: EnvironmentArguments\\n    agent: AgentArguments\\n    actions: ActionsArguments\\n    # Only run instances that completely match this regex\\n    instance_filter: str = \".*\"\\n    # Skip instances with existing trajectories\\n    skip_existing: bool = True\\n    # Suffix for the run name (used for example in trajectory directory naming)\\n    suffix: str = \"\"\\n    # Raise unhandled exceptions during the run (useful for debugging)\\n    raise_exceptions: bool = False\\n\\n    @property\\n    def run_name(self):\\n        \"\"\"Generate a unique name for this run based on the arguments.\"\"\"\\n        model_name = self.agent.model.model_name.replace(\":\", \"-\")\\n        data_stem = get_data_path_name(self.environment.data_path)\\n        assert self.agent.config_file is not None  # mypy\\n        config_stem = Path(self.agent.config_file).stem\\n\\n        temp = self.agent.model.temperature\\n        top_p = self.agent.model.top_p\\n\\n        per_instance_cost_limit = self.agent.model.per_instance_cost_limit\\n        install_env = self.environment.install_environment\\n\\n        return (\\n            f\"{model_name}__{data_stem}__{config_stem}__t-{temp:.2f}__p-{top_p:.2f}\"\\n            + f\"__c-{per_instance_cost_limit:.2f}__install-{int(install_env)}\"\\n            + (f\"__{self.suffix}\" if self.suffix else \"\")\\n        )\\n\\n\\nclass _ContinueLoop(Exception):\\n    \"\"\"Used for internal control flow\"\"\"\\n\\n\\nclass MainHook:\\n    \"\"\"Hook structure for the web server or other addons to interface with\"\"\"\\n\\n    @staticmethod\\n    def _is_promising_patch(info: dict[str, Any]) -> bool:\\n        \"\"\"Do we actually believe that the patch will solve the issue?\\n        Or are we just submitting the last patch we generated before hitting an error?\\n        \"\"\"\\n        # The exit status can also be `submitted (exit_cost)` etc.\\n        return info[\"exit_status\"] == \"submitted\" and info.get(\"submission\") is not None\\n\\n    def on_init(self, *, args: ScriptArguments, agent: Agent, env: SWEEnv, traj_dir: Path):\\n        \"\"\"Called when hook is initialized\"\"\"\\n\\n    def on_start(self):\\n        \"\"\"Called at the beginning of `Main.main`\"\"\"\\n\\n    def on_end(self):\\n        \"\"\"Called at the end of `Main.main`\"\"\"\\n\\n    def on_instance_start(self, *, index: int, instance: dict[str, Any]):\\n        \"\"\"Called at the beginning of each instance loop in `Main.run`\"\"\"\\n\\n    def on_instance_skipped(\\n        self,\\n    ):\\n        \"\"\"Called when an instance is skipped in `Main.run`\"\"\"\\n\\n    def on_instance_completed(self, *, info, trajectory):\\n        \"\"\"Called when an instance is completed in `Main.run`\"\"\"\\n\\n\\nclass SaveApplyPatchHook(MainHook):\\n    \"\"\"This hook saves patches to a separate directory and optionally applies them to a local repository.\"\"\"\\n\\n    def on_init(self, *, args: ScriptArguments, agent: Agent, env: SWEEnv, traj_dir: Path):\\n        self._traj_dir = traj_dir\\n        self._apply_patch_locally = args.actions.apply_patch_locally\\n        self._instance = None\\n\\n    def on_instance_start(self, *, index: int, instance: dict[str, Any]):\\n        self._instance = instance\\n\\n    def on_instance_completed(self, *, info, trajectory):\\n        assert self._instance is not None  # mypy\\n        instance_id = self._instance[\"instance_id\"]\\n        patch_path = self._save_patch(instance_id, info)\\n        if patch_path:\\n            if not self._apply_patch_locally:\\n                return\\n            if not self._is_promising_patch(info):\\n                return\\n            assert self._instance  # mypy\\n            if self._instance[\"repo_type\"] != \"local\":\\n                return\\n            local_dir = Path(self._instance[\"repo\"])\\n            self._apply_patch(patch_path, local_dir)\\n\\n    @staticmethod\\n    def _print_patch_message(patch_output_file: Path):\\n        console = rich.console.Console()\\n        msg = [\\n            \"SWE-agent has produced a patch that it believes will solve the issue you submitted!\",\\n            \"Use the code snippet below to inspect or apply it!\",\\n        ]\\n        panel = rich.panel.Panel.fit(\\n            \"\\\\n\".join(msg),\\n            title=\" Submission successful \",\\n        )\\n        console.print(panel)\\n        content = [\\n            \"```bash\",\\n            \"# The patch has been saved to your local filesystem at:\",\\n            f\"PATCH_FILE_PATH=\\'{patch_output_file.resolve()}\\'\",\\n            \"# Inspect it:\",\\n            \\'cat \"${PATCH_FILE_PATH}\"\\',\\n            \"# Apply it to a local repository:\",\\n            \"cd <your local repo root>\",\\n            \\'git apply \"${PATCH_FILE_PATH}\"\\',\\n            \"```\",\\n        ]\\n        console.print(rich.markdown.Markdown(\"\\\\n\".join(content)))\\n\\n    def _save_patch(self, instance_id: str, info) -> Path | None:\\n        \"\"\"Create patch files that can be applied with `git am`.\\n\\n        Returns:\\n            The path to the patch file, if it was saved. Otherwise, returns None.\\n        \"\"\"\\n        patch_output_dir = self._traj_dir / \"patches\"\\n        patch_output_dir.mkdir(exist_ok=True, parents=True)\\n        patch_output_file = patch_output_dir / f\"{instance_id}.patch\"\\n        if not info.get(\"submission\"):\\n            logger.info(\"No patch to save.\")\\n            return None\\n        model_patch = info[\"submission\"]\\n        patch_output_file.write_text(model_patch)\\n        if self._is_promising_patch(info):\\n            # Only print big congratulations if we actually believe\\n            # the patch will solve the issue\\n            self._print_patch_message(patch_output_file)\\n        return patch_output_file\\n\\n    def _apply_patch(self, patch_file: Path, local_dir: Path) -> None:\\n        \"\"\"Apply a patch to a local directory.\"\"\"\\n\\n        assert local_dir.is_dir()\\n        assert patch_file.exists()\\n        # The resolve() is important, because we\\'re gonna run the cmd\\n        # somewhere else\\n        cmd = [\"git\", \"apply\", str(patch_file.resolve())]\\n        try:\\n            subprocess.run(cmd, cwd=local_dir, check=True)\\n        except subprocess.CalledProcessError as e:\\n            logger.error(f\"Failed to apply patch {patch_file} to {local_dir}: {e}\")\\n            return\\n        logger.info(f\"Applied patch {patch_file} to {local_dir}\")\\n\\n\\nclass OpenPRHook(MainHook):\\n    \"\"\"This hook opens a PR if the issue is solved and the user has enabled the option.\"\"\"\\n\\n    def on_init(self, *, args: ScriptArguments, agent: Agent, env: SWEEnv, traj_dir: Path):\\n        self._env = env\\n        self._token: str = env._github_token\\n        self._data_path = args.environment.data_path\\n        self._open_pr = args.actions.open_pr\\n        self._skip_if_commits_reference_issue = args.actions.skip_if_commits_reference_issue\\n\\n    def on_instance_completed(self, *, info, trajectory):\\n        if self._open_pr and self.should_open_pr(info):\\n            self._env.open_pr(trajectory=trajectory)\\n\\n    def should_open_pr(self, info: dict[str, Any]) -> bool:\\n        \"\"\"Does opening a PR make sense?\"\"\"\\n        if not info.get(\"submission\"):\\n            logger.info(\"Not opening PR because no submission was made.\")\\n            return False\\n        if info[\"exit_status\"] != \"submitted\":\\n            logger.info(\"Not opening PR because exit status was %s and not submitted.\", info[\"exit_status\"])\\n            return False\\n        try:\\n            issue = get_gh_issue_data(self._data_path, token=self._token)\\n        except InvalidGithubURL:\\n            logger.info(\"Currently only GitHub is supported to open PRs to. Skipping PR creation.\")\\n            return False\\n        if issue.state != \"open\":\\n            logger.info(f\"Issue is not open (state={issue.state}. Skipping PR creation.\")\\n            return False\\n        if issue.assignee:\\n            logger.info(\"Issue is already assigned. Skipping PR creation. Be nice :)\")\\n            return False\\n        if issue.locked:\\n            logger.info(\"Issue is locked. Skipping PR creation.\")\\n            return False\\n        org, repo, issue_number = parse_gh_issue_url(self._data_path)\\n        associated_commits = get_associated_commit_urls(org, repo, issue_number, token=self._token)\\n        if associated_commits:\\n            commit_url_strs = \", \".join(associated_commits)\\n            if self._skip_if_commits_reference_issue:\\n                logger.info(f\"Issue already has associated commits (see {commit_url_strs}). Skipping PR creation.\")\\n                return False\\n            else:\\n                logger.warning(\\n                    \"Proceeding with PR creation even though there are already commits \"\\n                    f\"({commit_url_strs}) associated with the issue. Please only do this for your own repositories \"\\n                    \"or after verifying that the existing commits do not fix the issue.\",\\n                )\\n        return True\\n\\n\\nclass Main:\\n    def __init__(self, args: ScriptArguments):\\n        logger.info(f\" Arguments: {args.dumps_yaml()}\")\\n        self.args = args\\n        self.agent = Agent(\"primary\", args.agent)\\n        self.env = SWEEnv(args.environment)\\n        self.traj_dir = Path(\"trajectories\") / Path(getuser()) / args.run_name\\n        self.traj_dir.mkdir(parents=True, exist_ok=True)\\n        self._save_arguments()\\n        default_hooks = [\\n            SaveApplyPatchHook(),\\n            OpenPRHook(),\\n        ]\\n        self.hooks: list[MainHook] = []\\n        for hook in default_hooks:\\n            self.add_hook(hook)\\n\\n    def add_hook(self, hook: MainHook):\\n        hook.on_init(args=self.args, agent=self.agent, env=self.env, traj_dir=self.traj_dir)\\n        self.hooks.append(hook)\\n\\n    def run(self, index):\\n        # Reset environment\\n        instance_id = self.env.data[index][\"instance_id\"]\\n        for hook in self.hooks:\\n            hook.on_instance_start(index=index, instance=self.env.data[index])\\n        assert isinstance(instance_id, str)  # mypy\\n        if self.should_skip(instance_id):\\n            for hook in self.hooks:\\n                hook.on_instance_skipped()\\n            raise _ContinueLoop\\n        logger.info(\"  Beginning task \" + str(index))\\n\\n        observation, info = self.env.reset(index)\\n        if info is None:\\n            raise _ContinueLoop\\n\\n        # Get info, patch information\\n        issue = getattr(self.env, \"query\", None)\\n        files = []\\n        assert self.env.record is not None  # mypy\\n        if \"patch\" in self.env.record:\\n            files = \"\\\\n\".join([f\"- {x.path}\" for x in PatchSet(self.env.record[\"patch\"]).modified_files])\\n        # Get test files, F2P tests information\\n        test_files = []\\n        if \"test_patch\" in self.env.record:\\n            test_patch_obj = PatchSet(self.env.record[\"test_patch\"])\\n            test_files = \"\\\\n\".join([f\"- {x.path}\" for x in test_patch_obj.modified_files + test_patch_obj.added_files])\\n        tests = \"\"\\n        if \"FAIL_endTO_PASS\" in self.env.record:\\n            tests = \"\\\\n\".join([f\"- {x}\" for x in self.env.record[\"FAIL_TO_PASS\"]])\\n\\n        setup_args = {\"issue\": issue, \"files\": files, \"test_files\": test_files, \"tests\": tests}\\n        info, trajectory = self.agent.run(\\n            setup_args=setup_args,\\n            env=self.env,\\n            observation=observation,\\n            traj_dir=self.traj_dir,\\n            return_type=\"info_trajectory\",\\n        )\\n        self._save_predictions(instance_id, info)\\n        for hook in self.hooks:\\n            hook.on_instance_completed(info=info, trajectory=trajectory)\\n\\n    def main(self):\\n        for hook in self.hooks:\\n            hook.on_start()\\n        for index in range(len(self.env.data)):\\n            try:\\n                self.run(index)\\n            except _ContinueLoop:\\n                continue\\n            except KeyboardInterrupt:\\n                logger.info(\"Exiting InterCode environment...\")\\n                self.env.close()\\n                break\\n            except SystemExit:\\n                logger.critical(\" Exiting because SystemExit was called\")\\n                self.env.close()\\n                logger.info(\"Container closed\")\\n                raise\\n            except Exception as e:\\n                traceback.print_exc()\\n                if self.args.raise_exceptions:\\n                    self.env.close()\\n                    raise e\\n                if self.env.record:\\n                    logger.warning(f\" Failed on {self.env.record[\\'instance_id\\']}: {e}\")\\n                else:\\n                    logger.warning(\" Failed on unknown instance\")\\n                self.env.reset_container()\\n                continue\\n        for hook in self.hooks:\\n            hook.on_end()\\n\\n    def _save_arguments(self) -> None:\\n        \"\"\"Save the arguments to a yaml file to the run\\'s trajectory directory.\"\"\"\\n        log_path = self.traj_dir / \"args.yaml\"\\n\\n        if log_path.exists():\\n            try:\\n                other_args = self.args.load_yaml(log_path)\\n                if self.args.dumps_yaml() != other_args.dumps_yaml():  # check yaml equality instead of object equality\\n                    logger.warning(\"**************************************************\")\\n                    logger.warning(\"Found existing args.yaml with different arguments!\")\\n                    logger.warning(\"**************************************************\")\\n            except Exception as e:\\n                logger.warning(f\"Failed to load existing args.yaml: {e}\")\\n\\n        with log_path.open(\"w\") as f:\\n            self.args.dump_yaml(f)\\n\\n    def should_skip(self, instance_id: str) -> bool:\\n        \"\"\"Check if we should skip this instance based on the instance filter and skip_existing flag.\"\"\"\\n        # Skip instances that don\\'t match the instance filter\\n        if re.match(self.args.instance_filter, instance_id) is None:\\n            logger.info(f\"Instance filter not matched. Skipping instance {instance_id}\")\\n            return True\\n\\n        # If flag is set to False, don\\'t skip\\n        if not self.args.skip_existing:\\n            return False\\n\\n        # Check if there\\'s an existing trajectory for this instance\\n        log_path = self.traj_dir / (instance_id + \".traj\")\\n        if log_path.exists():\\n            with log_path.open(\"r\") as f:\\n                data = json.load(f)\\n            # If the trajectory has no exit status, it\\'s incomplete and we will redo it\\n            exit_status = data[\"info\"].get(\"exit_status\", None)\\n            if exit_status == \"early_exit\" or exit_status is None:\\n                logger.info(f\"Found existing trajectory with no exit status: {log_path}\")\\n                logger.info(\"Removing incomplete trajectory...\")\\n                os.remove(log_path)\\n            else:\\n                logger.info(f\" Skipping existing trajectory: {log_path}\")\\n                return True\\n        return False\\n\\n    def _save_predictions(self, instance_id: str, info):\\n        output_file = self.traj_dir / \"all_preds.jsonl\"\\n        model_patch = info[\"submission\"] if \"submission\" in info else None\\n        datum = {\\n            KEY_MODEL: Path(self.traj_dir).name,\\n            KEY_INSTANCE_ID: instance_id,\\n            KEY_PREDICTION: model_patch,\\n        }\\n        with open(output_file, \"a+\") as fp:\\n            print(json.dumps(datum), file=fp, flush=True)\\n        logger.info(f\"Saved predictions to {output_file}\")\\n\\n\\ndef get_args(args=None) -> ScriptArguments:\\n    \"\"\"Parse command line arguments and return a ScriptArguments object.\\n\\n    Args:\\n        args: Optional list of arguments to parse. If not provided, uses sys.argv.\\n    \"\"\"\\n    defaults = ScriptArguments(\\n        suffix=\"\",\\n        environment=EnvironmentArguments(\\n            image_name=\"sweagent/swe-agent:latest\",\\n            data_path=\"princeton-nlp/SWE-bench_Lite\",\\n            split=\"dev\",\\n            verbose=True,\\n            install_environment=True,\\n            cache_task_images=False,\\n        ),\\n        skip_existing=True,\\n        agent=AgentArguments(\\n            model=ModelArguments(\\n                model_name=\"gpt4\",\\n                total_cost_limit=0.0,\\n                per_instance_cost_limit=3.0,\\n                temperature=0.0,\\n                top_p=0.95,\\n            ),\\n            config_file=CONFIG_DIR / \"default.yaml\",\\n        ),\\n        actions=ActionsArguments(open_pr=False, skip_if_commits_reference_issue=True),\\n    )\\n\\n    # Nicer yaml dumping of multiline strings\\n    def multiline_representer(dumper, data):\\n        \"\"\"configures yaml for dumping multiline strings\\n        Ref: https://stackoverflow.com/questions/8640959/how-can-i-control-what-scalar-form-pyyaml-uses-for-my-data\\n        \"\"\"\\n        if data.count(\"\\\\n\") > 0:  # check for multiline string\\n            return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"|\")\\n        return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data)\\n\\n    yaml.add_representer(str, multiline_representer)\\n\\n    return parse(\\n        ScriptArguments,\\n        default=defaults,\\n        add_config_path_arg=False,\\n        args=args,\\n        formatter_class=RichHelpFormatter,\\n        description=Markdown(__doc__),\\n    )\\n\\n\\ndef main(args: ScriptArguments):\\n    Main(args).main()\\n\\n\\nif __name__ == \"__main__\":\\n    main(get_args())\\n\\n\\n**IMPORTANT**:\\nTest cases should be as specific as possible and written in a concise way with a maximum of around 50 tokens.\\n\\nReturn your answer in JSON format as such:\\n{\\n    \"test_file\": \"yes\",\\n    \"test_cases\": {\\n        \"Class or Function name 1\": [\"test case description\", \"test case description\", \"test case description\"],\\n        \"Class or Function name 2 \": [\"test case description\", \"test case description\", \"test case description\"],\\n    }\\n}\\n\\nIn case you think the file is not relevant for testing, return it as such:\\n{\\n    \"test_file\": \"no\",\\n    \"test_cases\": {}\\n}\\n'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_test_cases.format(file_content=agent.context.codebase.get_file_content(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {}\n",
    "for priority, files in files_to_test.items():\n",
    "    if priority != \"Low\":\n",
    "        for path in files:\n",
    "            prompt = extract_test_cases.format(file_content=agent.context.codebase.get_file_content(path))\n",
    "            messages[path] = agent.get_messages(prompt=prompt, add_context=False)\n",
    "            # responses = agent.generate_responses(messages, GPT4_BASE_PARAMS)\n",
    "            # print(responses[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026674"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.calculate_cost(messages, openai_params=GPT35_BASE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = agent.generate_responses(messages, GPT35_BASE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = {}\n",
    "for path, response in zip(messages.keys(), responses):\n",
    "    cases[path] = {}\n",
    "    for class_or_function, test_cases in eval(response[\"content\"])[\"test_cases\"].items():\n",
    "        cases[path][class_or_function] = test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../SWE-agent/run.py\n",
      "\tActionsArguments\n",
      "\t\t- Test that the post-init method raises a ValueError if push_gh_repo_url is provided\n",
      "\tScriptArguments\n",
      "\t\t- Test the run_name property to ensure it generates a unique name based on the arguments\n",
      "\tSaveApplyPatchHook\n",
      "\t\t- Test the _save_patch method to ensure it saves patch files correctly\n",
      "\t\t- Test the _apply_patch method to ensure it applies patches to a local directory correctly\n",
      "\tOpenPRHook\n",
      "\t\t- Test the should_open_pr method to determine if opening a PR makes sense based on the submission and issue status\n",
      "../../SWE-agent/inspector/server.py\n",
      "\tappend_exit\n",
      "\t\t- should return content as is if last entry role is 'system'\n",
      "\t\t- should append submission to history if exit status starts with 'submitted'\n",
      "\t\t- should raise ValueError if no submission in history or info\n",
      "\tappend_patch\n",
      "\t\t- should append gold patch if exit status is not None and instance ID in gold patches\n",
      "\t\t- should append test patch if exit status is not None and instance ID in test patches\n",
      "\tappend_results\n",
      "\t\t- should calculate and append instance stats to content\n",
      "\t\t- should determine and append status based on evaluation results\n",
      "\t\t- should append test results if scorecards are not None\n",
      "\tload_content\n",
      "\t\t- should load content from file and append exit, gold patch, and test patch to it\n",
      "\tload_results\n",
      "\t\t- should load results from results.json file and standardize 'no_generation' to 'not_generated' if necessary\n",
      "\tget_status\n",
      "\t\t- should return correct emoji based on results for single trajectory\n",
      "\tHandler\n",
      "\t\t- should serve directory info when path is '/directory_info'\n",
      "\t\t- should serve file content when path starts with '/trajectory/'\n",
      "\t\t- should handle files request and return sorted list of files with status\n",
      "\t\t- should check for updates and return appropriate response\n",
      "../../SWE-agent/sweagent/agent/models.py\n",
      "\tModelArguments\n",
      "\t\t- Test default values for model arguments\n",
      "\t\t- Test setting different values for model arguments\n",
      "\tAPIStats\n",
      "\t\t- Test addition of APIStats objects\n",
      "\t\t- Test replacing APIStats objects\n",
      "\tContextWindowExceededError\n",
      "\t\t- Test raising ContextWindowExceededError exception\n",
      "\tCostLimitExceededError\n",
      "\t\t- Test raising CostLimitExceededError exception\n",
      "\tBaseModel\n",
      "\t\t- Test initialization with ModelArguments and commands\n",
      "\t\t- Test resetting stats\n",
      "\t\t- Test updating stats\n",
      "\t\t- Test querying with history\n",
      "\tOpenAIModel\n",
      "\t\t- Test initialization with ModelArguments and commands\n",
      "\t\t- Test querying with history\n",
      "\tAnthropicModel\n",
      "\t\t- Test initialization with ModelArguments and commands\n",
      "\t\t- Test querying with history\n",
      "\tBedrockModel\n",
      "\t\t- Test initialization with ModelArguments and commands\n",
      "\t\t- Test querying with history\n",
      "\tOllamaModel\n",
      "\t\t- Test initialization with ModelArguments and commands\n",
      "\t\t- Test querying with history\n",
      "\tTogetherModel\n",
      "\t\t- Test initialization with ModelArguments and commands\n",
      "\t\t- Test querying with history\n",
      "\tHumanModel\n",
      "\t\t- Test initialization with ModelArguments and commands\n",
      "\t\t- Test querying with history\n",
      "\tHumanThoughtModel\n",
      "\t\t- Test initialization with ModelArguments and commands\n",
      "\t\t- Test querying with history\n",
      "\tReplayModel\n",
      "\t\t- Test initialization with ModelArguments and commands\n",
      "\t\t- Test querying with history\n",
      "\tInstantEmptySubmitTestModel\n",
      "\t\t- Test initialization with ModelArguments and commands\n",
      "\t\t- Test querying with history\n",
      "../../SWE-agent/sweagent/agent/agents.py\n",
      "\tSubroutine\n",
      "\t\t- Test if Subroutine class is frozen and has the expected attributes\n",
      "\tAgentConfig\n",
      "\t\t- Test if AgentConfig class is frozen and has the expected attributes\n",
      "\tAgentArguments\n",
      "\t\t- Test if AgentArguments class is frozen and has the expected attributes\n",
      "\tTrajectoryStep\n",
      "\t\t- Test if TrajectoryStep TypedDict has the expected keys\n",
      "\tAgentHook\n",
      "\t\t- Test if AgentHook class has all the expected methods\n",
      "\tAgent\n",
      "\t\t- Test if Agent class initializes correctly with the provided arguments\n",
      "\t\t- Test if Agent class can run on an environment and return the final value of the specified return type\n",
      "../../SWE-agent/sweagent/agent/parsing.py\n",
      "\tActionParser\n",
      "\t\t- Valid model response with a recognized command\n",
      "\t\t- Invalid model response with an unrecognized command\n",
      "\tThoughtActionParser\n",
      "\t\t- Valid model response with discussion and command in backticks\n",
      "\t\t- Invalid model response missing discussion or command tags\n",
      "\tXMLThoughtActionParser\n",
      "\t\t- Valid model response with discussion and command in XML tags\n",
      "\t\t- Invalid model response missing XML tags\n",
      "\tEditFormat\n",
      "\t\t- Valid model response with comments and replacement text in backticks\n",
      "\t\t- Invalid model response missing backticks for replacement text\n",
      "\tIdentity\n",
      "\t\t- Valid model response\n",
      "\t\t- Invalid model response\n",
      "../../SWE-agent/sweagent/environment/swe_env.py\n",
      "\tEnvironmentArguments\n",
      "\t\t- Test default values initialization\n",
      "\t\t- Test custom values initialization\n",
      "\t\t- Test post init method\n",
      "\tSWEEnv\n",
      "\t\t- Test environment initialization\n",
      "\t\t- Test reset method\n",
      "\t\t- Test step method\n",
      "\t\t- Test close method\n",
      "\t\t- Test interrupt method\n",
      "\t\t- Test open_pr method\n",
      "\tEnvHook\n",
      "\t\t- Test on_init method\n",
      "\t\t- Test on_copy_repo_started method\n",
      "\t\t- Test on_install_env_started method\n",
      "\t\t- Test on_close method\n",
      "../../SWE-agent/sweagent/environment/utils.py\n",
      "\tget_data_path_name\n",
      "\t\t- Test when data_path starts with 'text://'\n",
      "\t\t- Test when data_path is a GitHub issue URL\n",
      "\t\t- Test when data_path is a file path\n",
      "\tis_github_issue_url\n",
      "\t\t- Test with a valid GitHub issue URL\n",
      "\t\t- Test with an invalid GitHub issue URL\n",
      "\t\t- Test with a regular URL\n",
      "\tis_github_repo_url\n",
      "\t\t- Test with a valid GitHub repo URL\n",
      "\t\t- Test with an invalid GitHub repo URL\n",
      "\t\t- Test with a regular URL\n",
      "\tcopy_file_to_container\n",
      "\t\t- Test copying a string to a Docker container\n",
      "\t\t- Test handling exceptions during copying\n",
      "\t\t- Test cleanup of temporary file\n",
      "\tcopy_anything_to_container\n",
      "\t\t- Test copying files or directories from host to container\n",
      "\t\t- Test handling non-existent host path\n",
      "\t\t- Test subprocess run error handling\n",
      "\tread_with_timeout\n",
      "\t\t- Test reading data from subprocess with timeout\n",
      "\t\t- Test subprocess exiting unexpectedly\n",
      "\t\t- Test timeout while reading from subprocess\n",
      "\tread_with_timeout_experimental\n",
      "\t\t- Test reading data from subprocess with timeout (experimental)\n",
      "\t\t- Test subprocess exiting unexpectedly (experimental)\n",
      "\t\t- Test timeout while reading from subprocess (experimental)\n",
      "\tget_background_pids\n",
      "\t\t- Test getting background process IDs\n",
      "\t\t- Test when bash process is not found\n",
      "\t\t- Test when other processes are running\n",
      "\t_get_non_persistent_container\n",
      "\t\t- Test starting a non-persistent container\n",
      "\t\t- Test unexpected output during container setup\n",
      "\t\t- Test alien processes attached or running\n",
      "\t_get_persistent_container\n",
      "\t\t- Test starting a persistent container\n",
      "\t\t- Test unexpected output during container setup\n",
      "\t\t- Test alien processes attached or running\n",
      "\tget_container\n",
      "\t\t- Test getting a container object\n",
      "\t\t- Test Docker daemon not running error handling\n",
      "\t\t- Test multiple images found error handling\n",
      "\timage_exists\n",
      "\t\t- Test checking if image exists\n",
      "\t\t- Test Docker daemon not running error handling\n",
      "\t\t- Test multiple images found error handling\n",
      "\tget_commit\n",
      "\t\t- Test getting commit object from GitHub API\n",
      "\t\t- Test getting commit object with a specific ref\n",
      "\t\t- Test base commit reference resolution\n",
      "\tparse_gh_issue_url\n",
      "\t\t- Test parsing a valid GitHub issue URL\n",
      "\t\t- Test parsing an invalid GitHub issue URL\n",
      "\t\t- Test parsing a regular URL\n",
      "\tparse_gh_repo_url\n",
      "\t\t- Test parsing a valid GitHub repo URL\n",
      "\t\t- Test parsing an invalid GitHub repo URL\n",
      "\t\t- Test parsing a regular URL\n",
      "\tget_gh_issue_data\n",
      "\t\t- Test getting GitHub issue data\n",
      "\t\t- Test getting GitHub issue data with a token\n",
      "\tget_problem_statement_from_github_issue\n",
      "\t\t- Test getting problem statement from GitHub issue\n",
      "\t\t- Test getting problem statement from GitHub issue with a token\n",
      "\tInstanceBuilder\n",
      "\t\t- Test setting problem statement from GitHub issue\n",
      "\t\t- Test setting problem statement from local file\n",
      "\t\t- Test setting problem statement from text\n",
      "\t\t- Test setting repo info from GitHub URL\n",
      "\t\t- Test setting repo info from local path\n",
      "\t\t- Test setting repo info\n",
      "\t\t- Test setting from dictionary\n",
      "\t\t- Test setting missing fields\n",
      "\t\t- Test validation\n",
      "\t\t- Test building instance\n",
      "\tget_instances\n",
      "\t\t- Test getting instances from a file path\n",
      "\t\t- Test getting instances from a directory path\n",
      "\t\t- Test getting instances from a JSON file\n",
      "\t\t- Test getting instances from a JSONL file\n",
      "\t\t- Test getting instances from HF datasets\n",
      "\tget_associated_commit_urls\n",
      "\t\t- Test getting associated commit URLs\n",
      "\t\t- Test getting associated commit URLs with a token\n",
      "\tremove_triple_backticks\n",
      "\t\t- Test removing triple backticks from text\n",
      "\tformat_trajectory_markdown\n",
      "\t\t- Test formatting a trajectory as markdown\n",
      "../../SWE-agent/sweagent/api/server.py\n",
      "\tensure_session_id_set\n",
      "\t\t- session ID should be set if not already present\n",
      "\tMainThread.run\n",
      "\t\t- main thread should run without errors\n",
      "\t\t- exceptions should be caught and logged\n",
      "\tMainThread.stop\n",
      "\t\t- thread should stop when called\n",
      "\tindex\n",
      "\t\t- should return index.html template\n",
      "\thandle_connect\n",
      "\t\t- should print 'Client connected' when client connects\n",
      "\twrite_env_yaml\n",
      "\t\t- should write YAML file with given data and return file path\n",
      "\trun\n",
      "\t\t- should start a new run with given configuration\n",
      "\t\t- should handle CORS preflight request\n",
      "\tstop\n",
      "\t\t- should stop the current computation\n",
      "\t_build_cors_preflight_response\n",
      "\t\t- should build CORS preflight response\n",
      "../../SWE-agent/sweagent/api/hooks.py\n",
      "\tStreamToSocketIO\n",
      "\t\t- Test writing message to stream\n",
      "\t\t- Test flushing stream\n",
      "\tWebUpdate\n",
      "\t\t- Test updating log message\n",
      "\t\t- Test updating banner message\n",
      "\t\t- Test updating agent feed\n",
      "\t\t- Test updating environment feed\n",
      "\t\t- Test finishing run\n",
      "\tMainUpdateHook\n",
      "\t\t- Test on_start method\n",
      "\t\t- Test on_end method\n",
      "\t\t- Test on_instance_completed method\n",
      "\tAgentUpdateHook\n",
      "\t\t- Test on_actions_generated method\n",
      "\t\t- Test on_sub_action_started method\n",
      "\t\t- Test on_sub_action_executed method\n",
      "\tEnvUpdateHook\n",
      "\t\t- Test on_close method\n",
      "../../SWE-agent/sweagent/api/utils.py\n",
      "\t_async_raise\n",
      "\t\t- Raise exception with valid thread id and exception type\n",
      "\t\t- Raise exception with invalid thread id\n",
      "\t\t- Raise exception with non-class exception type\n",
      "\tThreadWithExc\n",
      "\t\t- Get thread id when thread is alive\n",
      "\t\t- Raise exception in the context of the thread\n",
      "\t\t- Ensure exception is caught by the thread\n",
      "\tstrip_ansi_sequences\n",
      "\t\t- Remove ANSI escape sequences from a string\n",
      "\t\t- Ensure all ANSI escape sequences are removed\n",
      "\t\t- Handle string with no ANSI escape sequences\n",
      "\tAttrDict\n",
      "\t\t- Access dictionary entries by attributes\n",
      "\t\t- Delete an attribute and access it as a key\n",
      "\t\t- Construct nested AttrDicts from nested dictionaries\n",
      "../../SWE-agent/sweagent/utils/config.py\n",
      "\tconvert_path_to_abspath\n",
      "\t\t- path is not absolute\n",
      "\t\t- path is absolute\n",
      "\t\t- SWE_AGENT_CONFIG_ROOT environment variable set\n",
      "\tconvert_paths_to_abspath\n",
      "\t\t- list of paths\n",
      "\t\t- all paths converted to absolute paths\n",
      "\tConfig.__init__\n",
      "\t\t- keys_cfg_path is None\n",
      "\t\t- keys.cfg file exists\n",
      "\t\t- keys.cfg file does not exist\n",
      "\tConfig.get\n",
      "\t\t- key in environment variables\n",
      "\t\t- key in keys.cfg\n",
      "\t\t- key not found\n",
      "\tConfig.__getitem__\n",
      "\t\t- key in environment variables\n",
      "\t\t- key in keys.cfg\n",
      "\t\t- key not found\n",
      "\tConfig.__contains__\n",
      "\t\t- key in environment variables\n",
      "\t\t- key in keys.cfg\n",
      "\t\t- key not found\n",
      "../../SWE-agent/sweagent/agent/commands.py\n",
      "\tAssistantMetadata\n",
      "\t\t- Test system_template is set to None by default\n",
      "\t\t- Test instance_template is set to None by default\n",
      "\tControlMetadata\n",
      "\t\t- Test next_step_template is set to None by default\n",
      "\t\t- Test next_step_action_template is set to None by default\n",
      "\tCommand\n",
      "\t\t- Test creating a Command instance with code, name, and optional docstring\n",
      "\t\t- Test creating a Command instance with arguments and signature\n",
      "\tParseCommand\n",
      "\t\t- Test get method returns a Command parser instance\n",
      "\t\t- Test NotImplementedError is raised when calling parse_command_file or generate_command_docs\n",
      "\tParseCommandBash\n",
      "\t\t- Test parsing a bash file with functions into a list of Command instances\n",
      "\t\t- Test parsing a bash script with @yaml docstring into a Command instance\n",
      "\tParseCommandDetailed\n",
      "\t\t- Test generating detailed documentation for a list of Command instances and subroutine types\n",
      "../../SWE-agent/sweagent/agent/history_processors.py\n",
      "\tHistoryProcessorMeta\n",
      "\t\t- Ensure new classes are added to the registry\n",
      "\tHistoryProcessor\n",
      "\t\t- Ensure NotImplementedError is raised when __call__ method is not implemented\n",
      "\t\t- Ensure get method raises ValueError when model output parser is not found\n",
      "\tDefaultHistoryProcessor\n",
      "\t\t- Ensure history is returned as is\n",
      "\tlast_n_history\n",
      "\t\t- Ensure ValueError is raised when n is less than or equal to 0\n",
      "\t\t- Ensure correct filtering of history based on n value\n",
      "\tLastNObservations\n",
      "\t\t- Ensure correct filtering of history based on n value\n",
      "\tLast2Observations\n",
      "\t\t- Ensure correct filtering of history based on n value\n",
      "\tLast5Observations\n",
      "\t\t- Ensure correct filtering of history based on n value\n",
      "\tClosedWindowHistoryProcessor\n",
      "\t\t- Ensure correct filtering of history based on window patterns\n",
      "../../SWE-agent/evaluation/aggregate_results.py\n",
      "\tget_folders\n",
      "\t\t- returns list of directories in a given path\n",
      "\tparse_folder_name\n",
      "\t\t- parses folder name into different parts\n",
      "\tconvert_experiments_to_rows\n",
      "\t\t- converts each experiment to a row in the csv\n",
      "\tget_results_df\n",
      "\t\t- returns a DataFrame of results from experiments\n",
      "\tget_results_csv\n",
      "\t\t- writes experiment results to a csv file\n",
      "\tmain\n",
      "\t\t- aggregates results from experiments based on filters\n",
      "../../SWE-agent/evaluation/evaluation.py\n",
      "\tmain\n",
      "\t\t- Check if predictions_path exists\n",
      "\t\t- Check if predictions_path is not empty\n",
      "\t\t- Check if evaluation run completes successfully\n",
      "\t\t- Check if evaluation run fails gracefully\n",
      "\t\t- Check if scorecards are generated correctly\n",
      "\t\t- Check if results are saved to files correctly\n",
      "\tget_eval_refs\n",
      "\t\t- Check if eval_refs dictionary is generated correctly\n",
      "\tget_logs_eval\n",
      "\t\t- Check if evaluation logs are extracted correctly\n",
      "\tget_model_report\n",
      "\t\t- Check if model report is generated correctly\n",
      "\tget_resolution_status\n",
      "\t\t- Check if resolution status is calculated correctly\n",
      "\trun_evaluation\n",
      "\t\t- Check if evaluation run is initiated correctly\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for path, response in zip(messages.keys(), responses):\n",
    "    print(path)\n",
    "    for class_or_function, test_cases in eval(response[\"content\"])[\"test_cases\"].items():\n",
    "        print(f\"\\t{class_or_function}\")\n",
    "        for test_case in test_cases:\n",
    "            print(f\"\\t\\t- {test_case}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritize test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "prioritize_test_cases = \"\"\"I want to generate some tests for an entire repository.\n",
    "The issue is that the repository is very large and I need to prioritize which tests should be generated first.\n",
    "I have already identified some test cases for the most important files in the repository and need you to prioritize which test cases should be implemented first.\n",
    "\n",
    "Here are the test cases that were generated for the most important files in the repository:\n",
    "{test_cases}\n",
    "\n",
    "Here are some descriptions of the content of these files:\n",
    "{full_descriptions_str}\n",
    "\n",
    "Based on the test cases and the file descriptions, prioritize which test cases should be implemented first.\n",
    "\n",
    "Return your answer in JSON format using the IDs from the test cases numbering as such:\n",
    "\"High\": [0, 1, 2],\n",
    "\"Medium\": [3, 4, 5],\n",
    "\"Low\": [6, 7, 8]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_prio_files = files_to_test[\"High\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[349], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m high_prio_test_cases \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcases\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m high_prio_files}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "high_prio_test_cases = {k: v for k, v in cases.items() if k in high_prio_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_prio_test_cases_ids = \"\"\n",
    "id_nr = 0\n",
    "for path, test_cases_dict in high_prio_test_cases.items():\n",
    "    high_prio_test_cases_ids += f\"{path}:\\n\"\n",
    "    for class_or_function, test_cases in test_cases_dict.items():\n",
    "        high_prio_test_cases_ids += f\"\\t{class_or_function}:\\n\"\n",
    "        for case in test_cases:\n",
    "            high_prio_test_cases_ids += f\"\\t\\t{id_nr} - {case}\\n\"\n",
    "            id_nr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_prio_descriptions = get_full_descriptions(high_prio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "prioritize_test_cases = prioritize_test_cases.format(test_cases=high_prio_test_cases_ids, full_descriptions_str=high_prio_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = agent.get_messages(prompt=prioritize_test_cases, add_context=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.015965"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.calculate_cost(messages, openai_params=GPT4_BASE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = agent.generate_responses(messages, GPT4_BASE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"High\": [\n",
      "    1, 2, 3, 4, 5, 10, 11, 16, 17, 18, 20, 21, 26, 27, 28, 29, 53, 54, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 99, 100, 101, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145\n",
      "  ],\n",
      "  \"Medium\": [\n",
      "    0, 6, 7, 8, 9, 12, 13, 14, 15, 19, 22, 23, 24, 25, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 74, 75, 76, 77, 102, 103, 104, 105, 106, 107\n",
      "  ],\n",
      "  \"Low\": [\n",
      "    9, 12, 13, 14, 15, 19, 22, 23, 24, 25, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 74, 75, 76, 77, 102, 103, 104, 105, 106, 107\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(responses[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define testing guidelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "define_guidelines = \"\"\"I want to generate a set of tests for an entire code repository.\n",
    "In order for these tests to be somewhat standardized, I first want to define some guidelines on how to generate these tests.\n",
    "These guidelines should include the testing framework to be used as well as any other information you think is relevant (naming, structure, etc.).\n",
    "\n",
    "Define the guidelines for the following files:\n",
    "{descriptions_and_tests}\n",
    "\n",
    "**IMPORTANT**:\n",
    "ONLY WRITE THE GUIDELINES. Be as concised as possible. Write the guidelines as bullet points. Try and limit them to 5-10 points.\n",
    "If you think multiple frameworks or guidelines should be used for different types of files, explain when each should be used.\n",
    "DO NOT write guidelines for each file separately, write them as general guidelines for the entire repository.\n",
    "Try not to exceed 200-300 tokens.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_and_tests = \"\"\n",
    "for path in files_to_test[\"High\"]:\n",
    "    descriptions_and_tests += path + \":\\n\"\n",
    "    descriptions_and_tests += \"Description:\\n\\t\" + descriptions_responses[path][\"description\"] + \"\\nTest cases:\\n\"\n",
    "    for class_or_function, test_cases in cases[path].items():\n",
    "        descriptions_and_tests += f\"\\t{class_or_function}:\\n\"\n",
    "        for test_case in test_cases:\n",
    "            descriptions_and_tests += f\"\\t\\t- {test_case}\\n\"\n",
    "    descriptions_and_tests += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "define_guidelines_prompt = define_guidelines.format(descriptions_and_tests=descriptions_and_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = agent.get_messages(prompt=define_guidelines_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = agent.generate_responses(messages, GPT4_NO_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_guidelines = responses[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Testing Framework**: Use `pytest` for all test cases. Utilize `unittest.mock` for mocking dependencies.\n",
      "- **Naming Conventions**: \n",
      "  - Test files should be named `test_<module_name>.py`.\n",
      "  - Test functions should be named `test_<functionality>`.\n",
      "- **Test Structure**:\n",
      "  - Group related tests using classes or modules.\n",
      "  - Use fixtures for setup and teardown.\n",
      "- **Assertions**: Use `assert` statements for checking expected outcomes.\n",
      "- **Coverage**: Aim for high code coverage; ensure critical paths and edge cases are tested.\n",
      "- **Mocking**: Mock external dependencies like API calls, file I/O, and subprocesses.\n",
      "- **Documentation**: Include docstrings in test functions to describe the purpose of the test.\n",
      "- **Isolation**: Ensure tests are independent and can run in any order.\n",
      "- **Error Handling**: Test both successful outcomes and expected failures/exceptions.\n",
      "- **CI Integration**: Ensure tests are integrated with Continuous Integration (CI) pipelines for automated testing.\n"
     ]
    }
   ],
   "source": [
    "print(test_guidelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "generare_tests = \"\"\"I want to generate a set of tests for a code repository.\n",
    "I have already generated the test cases for each file in the repository.\n",
    "\n",
    "Here is the file I want to write the tests for:\n",
    "{file_content}\n",
    "\n",
    "Generate tests that cover the following test cases:\n",
    "{test_cases}\n",
    "\n",
    "Use the following testing guidelines:\n",
    "{test_guidelines}\n",
    "\n",
    "**IMPORTANT**:\n",
    "Include the test case as docsctring in the corresponding test.\n",
    "ONLY WRITE THE CODE. Your output will directly be written to files as part of the repository, and will be executed as such.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../SWE-agent/run.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = agent.context.codebase.get_file_content(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases_str = \"\"\n",
    "for class_or_function, test_cases in cases[path].items():\n",
    "    test_cases_str += f\"{class_or_function}:\\n\"\n",
    "    for test_case in test_cases:\n",
    "        test_cases_str += f\"\\t- {test_case}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActionsArguments:\n",
      "\t- Test that the post-init method raises a ValueError if push_gh_repo_url is provided\n",
      "ScriptArguments:\n",
      "\t- Test the run_name property to ensure it generates a unique name based on the arguments\n",
      "SaveApplyPatchHook:\n",
      "\t- Test the _save_patch method to ensure it saves patch files correctly\n",
      "\t- Test the _apply_patch method to ensure it applies patches to a local directory correctly\n",
      "OpenPRHook:\n",
      "\t- Test the should_open_pr method to determine if opening a PR makes sense based on the submission and issue status\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_cases_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "generare_tests_prompt = generare_tests.format(file_content=file_content, test_cases=test_cases_str, test_guidelines=test_guidelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = agent.get_messages(prompt=generare_tests_prompt, add_context=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02405"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.calculate_cost(messages, openai_params=GPT4_NO_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = agent.generate_responses(messages, GPT4_NO_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "# test_run_dev.py\n",
      "\n",
      "import pytest\n",
      "from unittest.mock import patch, MagicMock, mock_open\n",
      "from run_dev import ActionsArguments, ScriptArguments, SaveApplyPatchHook, OpenPRHook, Main, get_args\n",
      "from pathlib import Path\n",
      "import subprocess\n",
      "import json\n",
      "import os\n",
      "\n",
      "# Test for ActionsArguments\n",
      "def test_ActionsArguments_post_init():\n",
      "    \"\"\"Test that the post-init method raises a ValueError if push_gh_repo_url is provided\"\"\"\n",
      "    with pytest.raises(ValueError, match=\"push_gh_repo_url is obsolete. Use repo_path instead\"):\n",
      "        ActionsArguments(push_gh_repo_url=\"some_url\")\n",
      "\n",
      "# Test for ScriptArguments\n",
      "def test_ScriptArguments_run_name():\n",
      "    \"\"\"Test the run_name property to ensure it generates a unique name based on the arguments\"\"\"\n",
      "    args = ScriptArguments(\n",
      "        environment=MagicMock(data_path=\"data_path\", install_environment=True),\n",
      "        agent=MagicMock(\n",
      "            model=MagicMock(\n",
      "                model_name=\"model_name\",\n",
      "                temperature=0.5,\n",
      "                top_p=0.9,\n",
      "                per_instance_cost_limit=10.0,\n",
      "            ),\n",
      "            config_file=\"config_file.yaml\"\n",
      "        ),\n",
      "        actions=MagicMock(),\n",
      "        suffix=\"test_suffix\"\n",
      "    )\n",
      "    expected_run_name = \"model_name__data_path__config_file__t-0.50__p-0.90__c-10.00__install-1__test_suffix\"\n",
      "    assert args.run_name == expected_run_name\n",
      "\n",
      "# Test for SaveApplyPatchHook\n",
      "@pytest.fixture\n",
      "def save_apply_patch_hook():\n",
      "    \"\"\"Fixture for SaveApplyPatchHook\"\"\"\n",
      "    return SaveApplyPatchHook()\n",
      "\n",
      "def test_SaveApplyPatchHook_save_patch(save_apply_patch_hook):\n",
      "    \"\"\"Test the _save_patch method to ensure it saves patch files correctly\"\"\"\n",
      "    instance_id = \"test_instance\"\n",
      "    info = {\"submission\": \"patch_content\"}\n",
      "    traj_dir = Path(\"/tmp/test_traj_dir\")\n",
      "    traj_dir.mkdir(parents=True, exist_ok=True)\n",
      "    save_apply_patch_hook._traj_dir = traj_dir\n",
      "\n",
      "    patch_path = save_apply_patch_hook._save_patch(instance_id, info)\n",
      "    assert patch_path == traj_dir / \"patches\" / f\"{instance_id}.patch\"\n",
      "    assert patch_path.read_text() == \"patch_content\"\n",
      "\n",
      "def test_SaveApplyPatchHook_apply_patch(save_apply_patch_hook):\n",
      "    \"\"\"Test the _apply_patch method to ensure it applies patches to a local directory correctly\"\"\"\n",
      "    patch_file = Path(\"/tmp/test_patch.patch\")\n",
      "    local_dir = Path(\"/tmp/test_local_dir\")\n",
      "    local_dir.mkdir(parents=True, exist_ok=True)\n",
      "    patch_file.write_text(\"patch_content\")\n",
      "\n",
      "    with patch(\"subprocess.run\") as mock_run:\n",
      "        save_apply_patch_hook._apply_patch(patch_file, local_dir)\n",
      "        mock_run.assert_called_once_with([\"git\", \"apply\", str(patch_file.resolve())], cwd=local_dir, check=True)\n",
      "\n",
      "# Test for OpenPRHook\n",
      "@pytest.fixture\n",
      "def open_pr_hook():\n",
      "    \"\"\"Fixture for OpenPRHook\"\"\"\n",
      "    return OpenPRHook()\n",
      "\n",
      "def test_OpenPRHook_should_open_pr(open_pr_hook):\n",
      "    \"\"\"Test the should_open_pr method to determine if opening a PR makes sense based on the submission and issue status\"\"\"\n",
      "    info = {\"submission\": \"some_submission\", \"exit_status\": \"submitted\"}\n",
      "    open_pr_hook._token = \"dummy_token\"\n",
      "    open_pr_hook._data_path = \"https://github.com/org/repo/issues/1\"\n",
      "    open_pr_hook._skip_if_commits_reference_issue = True\n",
      "\n",
      "    with patch(\"run_dev.get_gh_issue_data\") as mock_get_gh_issue_data, \\\n",
      "         patch(\"run_dev.get_associated_commit_urls\") as mock_get_associated_commit_urls, \\\n",
      "         patch(\"run_dev.parse_gh_issue_url\") as mock_parse_gh_issue_url:\n",
      "        \n",
      "        mock_get_gh_issue_data.return_value = MagicMock(state=\"open\", assignee=None, locked=False)\n",
      "        mock_get_associated_commit_urls.return_value = []\n",
      "        mock_parse_gh_issue_url.return_value = (\"org\", \"repo\", 1)\n",
      "        \n",
      "        assert open_pr_hook.should_open_pr(info) == True\n",
      "\n",
      "        mock_get_associated_commit_urls.return_value = [\"commit1\"]\n",
      "        assert open_pr_hook.should_open_pr(info) == False\n",
      "\n",
      "# Ensure the tests can run independently\n",
      "def test_main_should_skip():\n",
      "    \"\"\"Test the should_skip method to ensure it correctly identifies instances to skip\"\"\"\n",
      "    args = MagicMock(instance_filter=\".*\", skip_existing=True)\n",
      "    main = Main(args)\n",
      "    main.traj_dir = Path(\"/tmp/test_traj_dir\")\n",
      "    main.traj_dir.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "    instance_id = \"test_instance\"\n",
      "    log_path = main.traj_dir / (instance_id + \".traj\")\n",
      "\n",
      "    with open(log_path, \"w\") as f:\n",
      "        json.dump({\"info\": {\"exit_status\": \"completed\"}}, f)\n",
      "\n",
      "    assert main.should_skip(instance_id) == True\n",
      "\n",
      "    os.remove(log_path)\n",
      "    assert main.should_skip(instance_id) == False\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(responses[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "divergen_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
