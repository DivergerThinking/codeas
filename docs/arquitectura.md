# Arquitectura de *codeas*

Este documento resume c√≥mo est√° organizada la herramienta **codeas**, cu√°les son los servicios y m√≥dulos principales y c√≥mo fluyen los datos entre ellos.

## Arquitectura global

1. **Entrada y experiencia de usuario**. El ejecutable `codeas` inicia la interfaz de Streamlit definida en `src/codeas/main.py`, la cual carga la p√°gina principal `ui/üè†_Home.py`. Desde all√≠ se navega por las p√°ginas de documentaci√≥n, despliegue, testing, refactorizaci√≥n, chat, gesti√≥n de prompts y uso (todas en `src/codeas/ui/pages`).
2. **Estado compartido de la sesi√≥n**. `codeas.core.state.State` centraliza la ruta del repositorio activo, la instancia del cliente LLM, los metadatos precargados y los filtros de archivos. El estado tambi√©n gestiona la tabla de archivos visibles, la lectura/escritura de salidas y la persistencia de filtros en `.codeas/filters.json`.
3. **Modelo del repositorio**. `codeas.core.repo.Repo` indexa todos los archivos, calcula su costo en tokens, aplica filtros `include/exclude` y expone las rutas inclu√≠das para los distintos casos de uso.
4. **Metadatos y enriquecimiento**. `codeas.core.metadata.RepoMetadata` coordina agentes especializados para clasificar cada archivo (`FileUsage`), generar descripciones, extraer detalles de c√≥digo y de pruebas, y persistir los resultados en `.codeas/metadata.json`.
5. **Recuperaci√≥n de contexto**. `codeas.core.retriever.ContextRetriever` decide qu√© archivos (o res√∫menes) enviar al LLM seg√∫n flags por dominio (UI, API, DB, etc.) y seg√∫n si se requieren descripciones o detalles estructurados.
6. **Casos de uso**. Cada funcionalidad de alto nivel (documentaci√≥n, despliegue, testing, refactorizaci√≥n) reside en `src/codeas/use_cases`. All√≠ se preparan contextos, prompts y modelos espec√≠ficos, y se registra el costo en `UsageTracker`.
7. **Agentes y clientes LLM**. `codeas.core.agent.Agent` encapsula la construcci√≥n de mensajes, el c√°lculo de costos/tokens (v√≠a `tokencost`) y la ejecuci√≥n en `codeas.core.llm.LLMClient` (OpenAI) o `codeas.core.clients.LLMClients` (OpenAI/Anthropic/Gemini). Los prompts de los casos de uso se parametrizan en `src/codeas/configs/prompts.py`, mientras que los prompts de metadatos residen en `src/codeas/core/metadata.py`. Las p√°ginas usan `configs/agents_configs.py` y `configs/llm_params.py` seg√∫n corresponda.
8. **Telemetr√≠a de uso**. `codeas.core.usage_tracker.UsageTracker` persiste estad√≠sticas de cada llamada (costos, recuentos y logs de chat) en `~/codeas/usage.json`, lo cual alimenta la p√°gina ‚ÄúUsage‚Äù.

## Configuraci√≥n y dependencias

- **Gesti√≥n de claves y modelos**. La carga de proveedores y modelos disponibles se centraliza en `src/codeas/configs/llm_params.py`, que define los par√°metros por defecto y las opciones expuestas en la UI. Las claves pueden declararse mediante variables de entorno o en la pantalla principal.
- **Prompts y plantillas**. Los prompts base se encuentran en `src/codeas/configs/prompts.py`, mientras que las configuraciones espec√≠ficas por caso de uso se agrupan en `src/codeas/configs/agents_configs.py`. Esto permite ajustar temperatura, n√∫mero de muestras y plantillas por dominio.
- **Dependencias de UI**. Los componentes reutilizables (tablas, paneles de filtros, selectores de modelos) residen en `src/codeas/ui/components` y consumen utilidades comunes desde `src/codeas/ui/utils.py` y `src/codeas/ui/ui_state.py`.

## Secuencia t√≠pica de ejecuci√≥n

1. El usuario selecciona un repositorio en la p√°gina de inicio y decide si desea precargar metadatos existentes o generarlos desde cero.
2. `State` instancia `Repo` para descubrir archivos y calcular costes de tokens aproximados; en paralelo, se cargan los filtros previos desde `.codeas`.
3. Cuando se ejecuta un caso de uso, `ContextRetriever` utiliza los metadatos y los filtros activos para construir el contexto textual. Dependiendo de la p√°gina, se a√±aden opciones como ‚Äúsolo estructura‚Äù o ‚Äúresumen detallado‚Äù.
4. El agente correspondiente construye el prompt combinando contexto, instrucciones y par√°metros de modelo. La petici√≥n se env√≠a v√≠a `LLMClient/LLMClients`, registrando el coste estimado y real en `UsageTracker`.
5. Las salidas se muestran en la UI con controles de aceptaci√≥n o descarte; si el usuario decide aplicarlas, `State` escribe los archivos o diffs seleccionados en el repositorio.

## Consideraciones de seguridad y trazabilidad

- Los filtros `include/exclude` evitan que archivos sensibles entren en el contexto. Esta selecci√≥n queda reflejada en `.codeas/filters.json` para auditor√≠as posteriores.
- `UsageTracker` registra modelo, par√°metros y tama√±o de contexto para cada operaci√≥n, facilitando revisiones de coste y cumplimiento.
- Las rutas de salida se almacenan en `.codeas` para que los artefactos generados puedan verificarse sin reconsultar el modelo, √∫til en entornos con conectividad limitada.
- La UI exige confirmaci√≥n antes de aplicar cambios en disco y puede ejecutarse en modo lectura (solo vista de contexto) para revisiones sin riesgo.

## Servicios y m√≥dulos clave

| Servicio / m√≥dulo | Responsabilidad principal |
| --- | --- |
| `src/codeas/main.py` | Lanza la aplicaci√≥n Streamlit y enlaza la CLI con la UI. |
| `src/codeas/ui/üè†_Home.py` + `ui/pages` | Componen la experiencia visual, muestran formularios para cada caso de uso (Docs, Test, Deploy, Refactor, Chat, Prompts) y disparan acciones sobre el `state`. |
| `src/codeas/core/state.py` | Mantiene el estado compartido (repo activo, metadatos, filtros, datos para tablas) y coordina operaciones auxiliares (lectura/escritura de salidas). |
| `src/codeas/core/repo.py` | Descubre archivos, calcula tokens y aplica filtros `include/exclude` con coincidencias flexibles. |
| `src/codeas/core/metadata.py` | Define los esquemas de metadatos, orquesta agentes para poblarlos y expone helpers de lectura/exportaci√≥n. |
| `src/codeas/core/retriever.py` | Construye el contexto textual a partir de archivos completos o res√∫menes seg√∫n flags sem√°nticos. |
| `src/codeas/core/agent.py` | Normaliza la interacci√≥n con LLMs: construye mensajes, ejecuta completions/previas y calcula costos/tokens. |
| `src/codeas/core/llm.py` y `core/clients.py` | Proveen clientes sincr√≥nicos/as√≠ncronos para OpenAI (y wrappers para Anthropic/Gemini) con par√°metros centralizados. |
| `src/codeas/core/usage_tracker.py` | Registra el uso por caso, los historiales de chat y las ejecuciones de generadores. |
| `src/codeas/use_cases/documentation.py` | Genera secciones de documentaci√≥n combinando `ContextRetriever` con prompts por secci√≥n. |
| `src/codeas/use_cases/deployment.py` | Define estrategias y plantillas Terraform a partir del contexto del repo. |
| `src/codeas/use_cases/testing.py` | Produce estrategias de pruebas tipadas y genera archivos de test siguiendo las gu√≠as retornadas. |
| `src/codeas/use_cases/refactoring.py` | Selecciona grupos de archivos, propone cambios y genera diffs listos para aplicar. |

## Diagrama l√≥gico

```mermaid
graph TD
    CLI[CLI codeas / Streamlit] --> UI[Interfaz Streamlit]
    UI -->|invoca acciones| STATE[State]
    STATE --> REPO[Repo]
    STATE --> META[RepoMetadata]
    REPO --> RETRIEVER[ContextRetriever]
    META --> RETRIEVER
    RETRIEVER --> USECASES[Casos de uso]
    USECASES -->|prompts y flujos| AGENT[Agent]
    AGENT -->|mensajes| LLMCLIENT[LLMClient / LLMClients]
    LLMCLIENT -->|respuestas + costo| AGENT
    AGENT -->|registro| TRACKER[UsageTracker]
    TRACKER --> UI
    META -. persistencia .- FS[(.codeas/metadata.json)]
    STATE -. filtros .- FS
```

El diagrama muestra c√≥mo la interfaz invoca acciones sobre el estado, el cual coordina el repositorio y los metadatos. Los casos de uso consumen el contexto generado y delegan la generaci√≥n de contenido en agentes que, a su vez, se comunican con los clientes LLM y registran el consumo de recursos.
